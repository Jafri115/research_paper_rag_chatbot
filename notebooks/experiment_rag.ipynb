{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline with a Local Phi-3 Model and ArXiv Dataset\n",
    "\n",
    "This notebook demonstrates how to build a complete Retrieval-Augmented Generation (RAG) system from scratch using a locally hosted `microsoft/Phi-3-mini-4k-instruct` model. The pipeline will ingest and index a subset of the ArXiv dataset, retrieve relevant academic papers based on a user's query, and generate a comprehensive answer.\n",
    "\n",
    "**Key Steps:**\n",
    "1.  **Setup**: Load the local Phi-3 model and create a text generation pipeline.\n",
    "2.  **Data Loading & Preprocessing**: Download the ArXiv dataset and prepare it for indexing.\n",
    "3.  **Indexing**: Split the documents, create embeddings, and store them in a FAISS vector store.\n",
    "4.  **Retrieval**: Create a retriever to find relevant document chunks for a given query.\n",
    "5.  **Generation**: Combine the retrieved context with the query and use the local Phi-3 model to generate an answer.\n",
    "6.  **Advanced Features**: Implement an optional reranking step to improve retrieval quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gsk_****_HIDDEN_API_KEY_****'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GROQ_API_KEY\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model_path = snapshot_download(repo_id=model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32md:\\git_projects\\machine-learning\\research_paper_rag_chatbot\\.venv-tf\\lib\\site-packages\\importlib_metadata\\__init__.py:838\u001b[0m, in \u001b[0;36mFastPath.mtime\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mOSError\u001b[39;00m):\n\u001b[1;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mst_mtime\n\u001b[0;32m    839\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup\u001b[38;5;241m.\u001b[39mcache_clear()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'C:\\\\Users\\\\wasif\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.9.13\\\\python39.zip'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, pipeline\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load the local Phi-3 model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Make sure you have downloaded the model to this cache location first.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load tokenizer and model\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1055\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32md:\\git_projects\\machine-learning\\research_paper_rag_chatbot\\.venv-tf\\lib\\site-packages\\transformers\\utils\\import_utils.py:2302\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2300\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module:\n\u001b[0;32m   2301\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2302\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2303\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   2304\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\git_projects\\machine-learning\\research_paper_rag_chatbot\\.venv-tf\\lib\\site-packages\\transformers\\utils\\import_utils.py:2330\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2329\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2330\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2331\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2332\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\git_projects\\machine-learning\\research_paper_rag_chatbot\\.venv-tf\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Union\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto_factory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     _BaseAutoBackboneClass,\n\u001b[0;32m     25\u001b[0m     _BaseAutoModelClass,\n\u001b[0;32m     26\u001b[0m     _LazyAutoMapping,\n\u001b[0;32m     27\u001b[0m     auto_class_update,\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CONFIG_MAPPING_NAMES\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32md:\\git_projects\\machine-learning\\research_paper_rag_chatbot\\.venv-tf\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:43\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoConfig, model_type_to_module_name, replace_list_option_in_docstrings\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GenerationMixin\n\u001b[0;32m     46\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     48\u001b[0m _T \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_T\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1055\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32md:\\git_projects\\machine-learning\\research_paper_rag_chatbot\\.venv-tf\\lib\\site-packages\\transformers\\utils\\import_utils.py:2302\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2300\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module:\n\u001b[0;32m   2301\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2302\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2303\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   2304\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\git_projects\\machine-learning\\research_paper_rag_chatbot\\.venv-tf\\lib\\site-packages\\transformers\\utils\\import_utils.py:2330\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2329\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2330\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2331\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2332\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\git_projects\\machine-learning\\research_paper_rag_chatbot\\.venv-tf\\lib\\site-packages\\transformers\\generation\\utils.py:29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     30\u001b[0m     Cache,\n\u001b[0;32m     31\u001b[0m     DynamicCache,\n\u001b[0;32m     32\u001b[0m     EncoderDecoderCache,\n\u001b[0;32m     33\u001b[0m     QuantizedCache,\n\u001b[0;32m     34\u001b[0m     StaticCache,\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     38\u001b[0m     check_python_requirements,\n\u001b[0;32m     39\u001b[0m     get_cached_module_file,\n\u001b[0;32m     40\u001b[0m     get_class_in_module,\n\u001b[0;32m     41\u001b[0m     resolve_trust_remote_code,\n\u001b[0;32m     42\u001b[0m )\n",
      "File \u001b[1;32md:\\git_projects\\machine-learning\\research_paper_rag_chatbot\\.venv-tf\\lib\\site-packages\\transformers\\cache_utils.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hqq_available():\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhqq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Quantizer \u001b[38;5;28;01mas\u001b[39;00m HQQQuantizer\n\u001b[1;32m---> 20\u001b[0m _is_torch_greater_or_equal_than_2_7 \u001b[38;5;241m=\u001b[39m \u001b[43mis_torch_greater_or_equal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2.7\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_dev\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCacheLayerMixin\u001b[39;00m(ABC):\n",
      "File \u001b[1;32md:\\git_projects\\machine-learning\\research_paper_rag_chatbot\\.venv-tf\\lib\\site-packages\\transformers\\utils\\import_utils.py:1236\u001b[0m, in \u001b[0;36mis_torch_greater_or_equal\u001b[1;34m(library_version, accept_dev)\u001b[0m\n\u001b[0;32m   1229\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_torch_greater_or_equal\u001b[39m(library_version: \u001b[38;5;28mstr\u001b[39m, accept_dev: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;124;03m    Accepts a library version and returns True if the current version of the library is greater than or equal to the\u001b[39;00m\n\u001b[0;32m   1233\u001b[0m \u001b[38;5;124;03m    given version. If `accept_dev` is True, it will also accept development versions (e.g. 2.7.0.dev20250320 matches\u001b[39;00m\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;124;03m    2.7.0).\u001b[39;00m\n\u001b[0;32m   1235\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43m_is_package_available\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1237\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accept_dev:\n",
      "File \u001b[1;32md:\\git_projects\\machine-learning\\research_paper_rag_chatbot\\.venv-tf\\lib\\site-packages\\transformers\\utils\\import_utils.py:57\u001b[0m, in \u001b[0;36m_is_package_available\u001b[1;34m(pkg_name, return_version)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m package_exists:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;66;03m# TODO: Once python 3.9 support is dropped, `importlib.metadata.packages_distributions()`\u001b[39;00m\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;66;03m# should be used here to map from package name to distribution names\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m         \u001b[38;5;66;03m# Primary method to get the package version\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m         package_version \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mPackageNotFoundError:\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;66;03m# Fallback method: Only for \"torch\" and versions containing \"dev\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m pkg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\importlib\\metadata.py:569\u001b[0m, in \u001b[0;36mversion\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mversion\u001b[39m(distribution_name):\n\u001b[0;32m    563\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;124;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;124;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\importlib\\metadata.py:542\u001b[0m, in \u001b[0;36mdistribution\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdistribution\u001b[39m(distribution_name):\n\u001b[0;32m    537\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[0;32m    538\u001b[0m \n\u001b[0;32m    539\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;124;03m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\importlib\\metadata.py:192\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m resolver \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_discover_resolvers():\n\u001b[0;32m    191\u001b[0m     dists \u001b[38;5;241m=\u001b[39m resolver(DistributionFinder\u001b[38;5;241m.\u001b[39mContext(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m--> 192\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdists\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dist \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dist\n",
      "File \u001b[1;32md:\\git_projects\\machine-learning\\research_paper_rag_chatbot\\.venv-tf\\lib\\site-packages\\importlib_metadata\\__init__.py:974\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Find metadata directories in paths heuristically.\"\"\"\u001b[39;00m\n\u001b[0;32m    972\u001b[0m prepared \u001b[38;5;241m=\u001b[39m Prepared(name)\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m--> 974\u001b[0m     \u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepared\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(FastPath, paths)\n\u001b[0;32m    975\u001b[0m )\n",
      "File \u001b[1;32md:\\git_projects\\machine-learning\\research_paper_rag_chatbot\\.venv-tf\\lib\\site-packages\\importlib_metadata\\__init__.py:833\u001b[0m, in \u001b[0;36mFastPath.search\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[1;32m--> 833\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmtime\u001b[49m)\u001b[38;5;241m.\u001b[39msearch(name)\n",
      "File \u001b[1;32md:\\git_projects\\machine-learning\\research_paper_rag_chatbot\\.venv-tf\\lib\\site-packages\\importlib_metadata\\__init__.py:838\u001b[0m, in \u001b[0;36mFastPath.mtime\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    835\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmtime\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    837\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mOSError\u001b[39;00m):\n\u001b[1;32m--> 838\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mst_mtime\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup\u001b[38;5;241m.\u001b[39mcache_clear()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load the local Phi-3 model\n",
    "# Make sure you have downloaded the model to this cache location first.\n",
    "# Load tokenizer and model\n",
    "print(\"Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Example text generation\n",
    "prompt = \"Once upon a time\"\n",
    "output = generator(prompt)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Dataset file located at: C:\\Users\\wasif\\.cache\\kagglehub\\datasets\\Cornell-University\\arxiv\\versions\\250\\arxiv-metadata-oai-snapshot.json\n",
      "Successfully loaded 50000 records.\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "print(\"Downloading dataset...\")\n",
    "# This requires kagglehub to be installed (pip install kagglehub) and your Kaggle API token to be set up.\n",
    "path = kagglehub.dataset_download(\"Cornell-University/arxiv\")\n",
    "data_file = os.path.join(path, \"arxiv-metadata-oai-snapshot.json\")\n",
    "print(f\"Dataset file located at: {data_file}\")\n",
    "\n",
    "# Load a subset of the data to manage memory usage\n",
    "def load_data_subset(file_path, num_records=50000):\n",
    "    records = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= num_records:\n",
    "                break\n",
    "            records.append(json.loads(line))\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "df = load_data_subset(data_file)\n",
    "print(f\"Successfully loaded {len(df)} records.\")\n",
    "\n",
    "# Preprocessing\n",
    "df['update_date'] = pd.to_datetime(df['update_date'])\n",
    "df['year'] = df['update_date'].dt.year\n",
    "df = df.dropna(subset=['abstract'])\n",
    "df = df[df['abstract'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 50000 Document objects.\n",
      "\n",
      "--- Sample Document ---\n",
      "Title: Calculation of prompt diphoton production cross sections at Tevatron and\n",
      "  LHC energies\n",
      "\n",
      "Abstract:   A fully differential calculation in perturbative quantum chromodynamics is\n",
      "presented for the production of massive photon pairs at hadron colliders. All\n",
      "next-to-leading order perturbative contributions from quark-antiquark,\n",
      "gluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\n",
      "all-orders resummation of initial-state gluon radiation valid at\n",
      "next-to-next-to-leading logari\n",
      "\n",
      "Metadata: {'id': '0704.0001', 'authors': \"C. Bal\\\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan\", 'year': 2008, 'categories': 'hep-ph'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Create LangChain Document objects for easier processing in the RAG pipeline\n",
    "documents = []\n",
    "for _, row in df.iterrows():\n",
    "    page_content = f\"Title: {row['title']}\\n\\nAbstract: {row['abstract']}\"\n",
    "    metadata = {\n",
    "        \"id\": row.get('id', 'N/A'),\n",
    "        \"authors\": row.get('authors', 'N/A'),\n",
    "        \"year\": row.get('year', 'N/A'),\n",
    "        \"categories\": row.get('categories', 'N/A')\n",
    "    }\n",
    "    documents.append(Document(page_content=page_content, metadata=metadata))\n",
    "\n",
    "print(f\"Created {len(documents)} Document objects.\")\n",
    "print(\"\\n--- Sample Document ---\")\n",
    "print(documents[0].page_content[:500])\n",
    "print(f\"\\nMetadata: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Core RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Indexing\n",
    "\n",
    "We will now process the loaded documents and store them in a vector database for efficient retrieval. We will use a local sentence-transformer model for creating embeddings and FAISS for the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing FAISS index from faiss_index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wasif\\AppData\\Local\\Temp\\ipykernel_12832\\1314418873.py:11: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "save_path = \"faiss_index\"\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    print(f\"Loading existing FAISS index from {save_path}...\")\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n",
    "    )\n",
    "    vectorstore = FAISS.load_local(\n",
    "        save_path,\n",
    "        embedding_model,\n",
    "        allow_dangerous_deserialization=True  # needed if pickle is used internally\n",
    "    )\n",
    "    print(\"Vector store loaded successfully.\")\n",
    "\n",
    "else:\n",
    "    print(\"No existing FAISS index found. Creating a new one...\")\n",
    "\n",
    "    # 1. Split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        add_start_index=True\n",
    "    )\n",
    "    all_splits = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(all_splits)} chunks.\")\n",
    "\n",
    "    # 2. Create embeddings\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n",
    "    )\n",
    "\n",
    "    # 3. Build vectorstore\n",
    "    print(\"\\nCreating vector store... This may take a few minutes.\")\n",
    "    vectorstore = FAISS.from_documents(documents=all_splits, embedding=embedding_model)\n",
    "    vectorstore.save_local(save_path)\n",
    "    print(f\"Vector store created and saved to {save_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store saved to faiss_index\n"
     ]
    }
   ],
   "source": [
    "# Save FAISS vectorstore to local directory\n",
    "save_path = \"faiss_index\"\n",
    "vectorstore.save_local(save_path)\n",
    "print(f\"Vector store saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Retrieval\n",
    "\n",
    "The retriever's job is to fetch the most relevant document chunks from the vector store based on the user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 documents for the query: 'What is the theory of sparsity-certifying graph decompositions?'\n",
      "\n",
      "--- Document 1 ---\n",
      "Title: Characterizing Sparse Graphs by Map Decompositions\n",
      "\n",
      "Abstract:   A {\\bf map} is a graph that admits an orientation of its edges so that each\n",
      "vertex has out-degree exactly 1. We characterize graphs which admit a\n",
      "decomposition into $k$ edge-disjoint maps after: (1) the addition of {\\it any}\n",
      "$\\el...\n",
      "Metadata: {'id': '0704.3843', 'authors': 'Ruth Haas, Audrey Lee, Ileana Streinu, and Louis Theran', 'year': 2011, 'categories': 'math.CO', 'start_index': 0}\n",
      "\n",
      "--- Document 2 ---\n",
      "Title: Sparsity-certifying Graph Decompositions\n",
      "\n",
      "Abstract:   We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use\n",
      "it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and\n",
      "algorithmic solutions to a family of problems concerning tree decompositions of\n",
      "graphs....\n",
      "Metadata: {'id': '0704.0002', 'authors': 'Ileana Streinu and Louis Theran', 'year': 2008, 'categories': 'math.CO cs.CG', 'start_index': 0}\n",
      "\n",
      "--- Document 3 ---\n",
      "Title: Metrics for sparse graphs...\n",
      "Metadata: {'id': '0708.1919', 'authors': 'B. Bollobas and O. Riordan', 'year': 2010, 'categories': 'math.CO math.PR', 'start_index': 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# Test the retriever with a sample query\n",
    "query = \"What is the theory of sparsity-certifying graph decompositions?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents for the query: '{query}'\\n\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"--- Document {i+1} ---\")\n",
    "    print(doc.page_content[:300] + \"...\")\n",
    "    print(f\"Metadata: {doc.metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Generation with Local Phi-3 Model\n",
    "\n",
    "Now we'll tie everything together into a chain. The chain will:\n",
    "1. Take the user's question.\n",
    "2. Use the retriever to find relevant context.\n",
    "3. Format the context and question into a prompt.\n",
    "4. Pass the prompt to our local Phi-3 model to generate the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "# Initialize Groq LLM (choose model from Groq's catalog, e.g. LLaMA-3 or Mixtral)\n",
    "llm = ChatGroq(\n",
    "    model=\"meta-llama/llama-4-maverick-17b-128e-instruct\",   # or \"mixtral-8x7b-32768\"\n",
    "    temperature=0.7,\n",
    "    max_tokens=512,\n",
    "    groq_api_key=os.environ[\"GROQ_API_KEY\"],\n",
    ")\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Helper function to format docs\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# Prompt template\n",
    "rag_prompt_template = \"\"\"Answer the following question based on the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=rag_prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Build RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing the Complete RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Query: What is the theory of sparsity-certifying graph decompositions?\n",
      "============================================================\n",
      "\n",
      "Answer:\n",
      "## Step 1: Understand the context of the question\n",
      "The question is asking about the theory of sparsity-certifying graph decompositions, which is related to the characterization and algorithmic solutions for sparse graphs.\n",
      "\n",
      "## Step 2: Review the provided abstracts to identify relevant information\n",
      "The abstracts of the papers \"Characterizing Sparse Graphs by Map Decompositions\" and \"Sparsity-certifying Graph Decompositions\" provide insights into the characterization of sparse graphs and the algorithms used for their decomposition.\n",
      "\n",
      "## Step 3: Identify the key elements of sparsity-certifying graph decompositions\n",
      "The abstract of \"Sparsity-certifying Graph Decompositions\" mentions the $(k,\\ell)$-pebble game with colors as a new algorithm for characterizing $(k,\\ell)$-sparse graphs and solving problems related to tree decompositions.\n",
      "\n",
      "## Step 4: Relate the $(k,\\ell)$-pebble game with colors to sparsity-certifying graph decompositions\n",
      "The $(k,\\ell)$-pebble game with colors is described as a tool for obtaining a characterization of the family of $(k,\\ell)$-sparse graphs and for providing algorithmic solutions to problems concerning tree decompositions.\n",
      "\n",
      "## Step 5: Formulate the answer based on the information gathered\n",
      "The theory of sparsity-certifying graph decompositions involves characterizing sparse graphs using algorithms such as the $(k,\\ell)$-pebble game with colors, and providing solutions to problems related to tree decompositions of graphs. It generalizes and strengthens previous results, such as those by Lee and Streinu, and provides a new proof of the Tutte-Nash-Williams characterization of arboricity.\n",
      "\n",
      "The final answer is: The $(k,\\ell)$-pebble game with colors is used to characterize $(k,\\ell)$-sparse graphs and provide algorithmic solutions to problems concerning tree decompositions, generalizing previous results and providing new proofs for known characterizations.\n",
      "\n",
      "--- Retrieved Documents ---\n",
      "Doc 1: Title: Characterizing Sparse Graphs by Map Decompositions\n",
      "\n",
      "Abstract:   A {\\bf map} is a graph that admits an orientation of its edges so that each\n",
      "vertex has out-degree exactly 1. We characterize grap...\n",
      "Doc 2: Title: Sparsity-certifying Graph Decompositions\n",
      "\n",
      "Abstract:   We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use\n",
      "it obtain a characterization of the family of $(k,\\ell)$-spars...\n",
      "\n",
      "============================================================\n",
      "Query: Tell me about recent advances in machine learning\n",
      "============================================================\n",
      "\n",
      "Answer:\n",
      "Based on the provided context, recent advances in machine learning include the development of kernel methods and ensemble methods. Kernel methods allow for the transformation of linear algorithms into nonlinear ones using kernel functions, while ensemble methods make predictions using a collection of models rather than a single model. Two specific examples of recent algorithms that have been invented include LAGO, a fast kernel algorithm for unbalanced classification and rare target detection, and \"Darwinian evolution in parallel universes\", an ensemble method for variable selection. These advances have been influenced by earlier work on support vector machines and the AdaBoost algorithm, which emerged in the 1990s and spawned a wave of research in statistical machine learning.\n",
      "\n",
      "--- Retrieved Documents ---\n",
      "Doc 1: Title: Kernels and Ensembles: Perspectives on Statistical Learning\n",
      "\n",
      "Abstract:   Since their emergence in the 1990's, the support vector machine and the\n",
      "AdaBoost algorithm have spawned a wave of resear...\n",
      "Doc 2: Title: Evolving Classifiers: Methods for Incremental Learning...\n",
      "\n",
      "============================================================\n",
      "Query: What are the applications of quantum computing?\n",
      "============================================================\n",
      "\n",
      "Answer:\n",
      "Based on the provided context, the applications of quantum computing are related to \"quantum information processing\". Specifically, it is mentioned that this approach \"opens new perspectives in quantum information processing, wherein both the data and the programs are represented by states of quantum registers.\" Additionally, the context from the first title, \"An Introduction to Quantum Computing\", mentions that quantum computing concerns the \"utilization of quantum mechanics to improve the efficiency of computation\". \n",
      "\n",
      "Therefore, the applications of quantum computing include improving the efficiency of computation and quantum information processing.\n",
      "\n",
      "--- Retrieved Documents ---\n",
      "Doc 1: Title: An Introduction to Quantum Computing\n",
      "\n",
      "Abstract:   Quantum Computing is a new and exciting field at the intersection of\n",
      "mathematics, computer science and physics. It concerns a utilization of qu...\n",
      "Doc 2: quantum processors. This approach opens new perspectives in quantum information\n",
      "processing, wherein both the data and the programs are represented by states of\n",
      "quantum registers. In particular, quantu...\n"
     ]
    }
   ],
   "source": [
    "# Test with a list of different queries\n",
    "test_queries = [\n",
    "    \"What is the theory of sparsity-certifying graph decompositions?\",\n",
    "    \"Tell me about recent advances in machine learning\",\n",
    "    \"What are the applications of quantum computing?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get the answer using the RAG chain\n",
    "    answer = rag_chain.invoke(query)\n",
    "    print(f\"\\nAnswer:\\n{answer.content if hasattr(answer, 'content') else answer}\")\n",
    "    \n",
    "    # Optionally, show the documents that were retrieved to generate the answer\n",
    "    print(f\"\\n--- Retrieved Documents ---\")\n",
    "    retrieved = retriever.invoke(query)\n",
    "    for i, doc in enumerate(retrieved[:2]):  # Show first 2 docs for brevity\n",
    "        print(f\"Doc {i+1}: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced RAG with Reranking (Optional)\n",
    "\n",
    "Sometimes, the initial retrieval can be noisy. A reranking step can improve the quality of the context provided to the LLM by re-sorting the initially retrieved documents based on a more fine-grained relevance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# A simple reranker class that uses the same embedding model to calculate cosine similarity\n",
    "class SimpleReranker:\n",
    "    def __init__(self, embedding_model):\n",
    "        self.embedding_model = embedding_model\n",
    "    \n",
    "    def rerank(self, query: str, documents: List[Document], top_k: int = 3):\n",
    "        # Get embeddings for the query\n",
    "        query_embedding = self.embedding_model.embed_query(query)\n",
    "        \n",
    "        # Calculate similarity scores for each document\n",
    "        scores = []\n",
    "        for doc in documents:\n",
    "            doc_embedding = self.embedding_model.embed_query(doc.page_content)\n",
    "            # Simple cosine similarity calculation\n",
    "            similarity = sum(a*b for a, b in zip(query_embedding, doc_embedding))\n",
    "            scores.append((similarity, doc))\n",
    "        \n",
    "        # Sort documents by similarity score in descending order and return the top_k\n",
    "        scores.sort(key=lambda x: x[0], reverse=True)\n",
    "        return [doc for _, doc in scores[:top_k]]\n",
    "\n",
    "# Create reranker instance\n",
    "reranker = SimpleReranker(embedding_model)\n",
    "\n",
    "# Enhanced retrieval function that includes a reranking step\n",
    "def retrieve_and_rerank(query, k_retrieve=6, k_rerank=3):\n",
    "    # Initial retrieval of a larger set of documents\n",
    "    initial_docs = retriever.invoke(query)\n",
    "    \n",
    "    # Rerank the initially retrieved documents\n",
    "    reranked_docs = reranker.rerank(query, initial_docs, top_k=k_rerank)\n",
    "    \n",
    "    return reranked_docs\n",
    "\n",
    "# Test the reranking process\n",
    "query = \"What are neural networks used for?\"\n",
    "reranked_docs = retrieve_and_rerank(query)\n",
    "print(f\"Reranked documents for: '{query}'\")\n",
    "for i, doc in enumerate(reranked_docs):\n",
    "    print(f\"\\nDoc {i+1}: {doc.page_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save and Load the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the created vector store to disk for future use, avoiding the need to re-index\n",
    "vectorstore.save_local(\"arxiv_vectorstore\")\n",
    "print(\"Vector store saved to 'arxiv_vectorstore'\")\n",
    "\n",
    "# To load it in a future session:\n",
    "# loaded_vectorstore = FAISS.load_local(\"arxiv_vectorstore\", embedding_model, allow_dangerous_deserialization=True)\n",
    "# loaded_retriever = loaded_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Batch Processing for Multiple Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_rag_query(queries: List[str]):\n",
    "    \"\"\"Process a list of queries in a batch and handle potential errors.\"\"\"\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        try:\n",
    "            answer = rag_chain.invoke(query)\n",
    "            results.append({\n",
    "                \"query\": query,\n",
    "                \"answer\": answer,\n",
    "                \"status\": \"success\"\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"query\": query,\n",
    "                \"answer\": None,\n",
    "                \"status\": f\"error: {str(e)}\"\n",
    "            })\n",
    "    return results\n",
    "\n",
    "# Test batch processing with a new set of queries\n",
    "batch_queries = [\n",
    "    \"What is deep learning?\",\n",
    "    \"Explain reinforcement learning\",\n",
    "    \"What are transformers in NLP?\"\n",
    "]\n",
    "\n",
    "batch_results = batch_rag_query(batch_queries)\n",
    "for result in batch_results:\n",
    "    print(f\"\\nQ: {result['query']}\")\n",
    "    print(f\"A: {result['answer'][:200] if result['answer'] else result['status']}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
